{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej",
        "colab_type": "text"
      },
      "source": [
        "#Part 4 BERT for arithmetic sentiment analysis\n",
        "\n",
        "Acknowledgement: We used most of the code from https://mccormickml.com/2019/07/22/BERT-fine-tuning/ \n",
        "\n",
        "Most Credit to: \n",
        "Chris McCormick and Nick Ryan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhPwblrwyDxj",
        "colab_type": "text"
      },
      "source": [
        "# Bert Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyytew4tyT8z",
        "colab_type": "text"
      },
      "source": [
        "**B**idirectional **E**ncoder **R**epresentations from\n",
        "**T**ransformers (BERT) [Devlin  et  al.,  2019], as the name suggests, is a language model based on the Transformer  [Vaswani et al., 2017] encoder architecture that has been pre-trained on a large dataset of unlabeled sentences from Wikipedia and BookCorpus [Zhu et al., 2015]. Given a sequence of tokens representing sentence(s), BERT outputs a ``contextualized representation\" vector for each of the token. Now, suppose we are given some down-stream tasks, such as sentence classification or question-answering. We can take the BERT model, add a small layer on top of the BERT representation(s), and then fine-tune the added parameters **and** BERT parameters on the down-stream dataset, which is typically much smaller than the data used to pre-train BERT. \n",
        "\n",
        "In traditional language modeling task, the objective is to maximize the log likelihood of predicting the current word (or token) in the sentence, given the previous words (to the left of current work) as context. This is called the *autoregressive model*. In BERT, however, we wish to predict the current word given both the words before and after (i.e. to the left and to the right) of the sentence--hence *bidirectional*.\n",
        "To be able to attend from both directions, BERT uses the encoder Transformer, which does not apply any attention masking unlike the decoder.\n",
        "\n",
        "We briefly describe how BERT is pre-trained. BERT has 2 task objectives for pre-training: (1) *Masked Language Modeling* (Masked LM), and (2) *Next Sentence Prediction*(NSP). The input to the model is a sequence of tokens of the form:\n",
        "```\n",
        "    [CLS] Sentence A [SEP] Sentence B,\n",
        "```\n",
        "where `[CLS]`  (\"class\") and `[SEP]` (\"separator\") are special tokens. \n",
        "In Masked LM, some percentage of the input tokens are converted into `[MASK]` tokens, and the objective is to use the final layer representation for that masked token to predict the correct word that was masked out. For NSP, the task is to use the contextualized representation for the `[CLS]` token to perform binary classification for whether sentence A and sentence B are consecutive sentences in the unlabeled dataset. See Figure 6 (in Handout) for the conceptual picture of BERT pre-training and fine-tuning. \n",
        "\n",
        "In this assignment, you will be **fine-tuning BERT on a single sentence classification task** (see below about the dataset). Figure 7 (in Handout) illustrates the architecture for fine-tuning on this task. We prepend the tokenized sentence with the `[CLS]` token, then feed the sequence into BERT. We then take the contextualized `[CLS]` token representation at the last layer of BERT and add either a softmax layer on top corresponding to the number of output classes in the task. Alternatively, we can have fully connected hidden layers before the softmax layer for more expressivity for harder tasks. Then, both the new layers and the entire BERT parameters are trained end to end on the task for a few epochs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI",
        "colab_type": "text"
      },
      "source": [
        "## Install transformers repo that has Bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab_type": "code",
        "outputId": "dff1374b-1fde-4ac2-8781-79e65fc722cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.18)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.18)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn",
        "colab_type": "text"
      },
      "source": [
        "## Download & Extract\n",
        "Run the following cells to downlaod the dataset files from the CSC413 webpage.\n",
        "\n",
        "<!-- Download the two csv dataset files from CSC413 webpage, click the folder icon,  -->\n",
        "<!-- and click \"upload\" to upload them.  -->\n",
        "\n",
        "<!-- https://csc413-2020.github.io/assets/misc/PA03_data_20_train.csv \n",
        "\n",
        "https://csc413-2020.github.io/assets/misc/PA03_data_20_test.csv   -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydihgBb-AiY",
        "colab_type": "code",
        "outputId": "2761736d-587e-4831-f696-ccb23d0b2b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W9K3PG3-C5W",
        "colab_type": "code",
        "outputId": "27b7cf62-3678-4b85-85e1-9949598bb791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading verbal arithmetic dataset')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://csc413-2020.github.io/assets/misc/'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./PA03_data_20_train.csv'):\n",
        "  wget.download(url + 'PA03_data_20_train.csv', './PA03_data_20_train.csv')\n",
        "  print('Done downloading training data')\n",
        "else:\n",
        "  print('Already downloaded training data')\n",
        "\n",
        "if not os.path.exists('./PA03_data_20_test.csv'):\n",
        "  wget.download(url + 'PA03_data_20_test.csv', './PA03_data_20_test.csv')\n",
        "  print('Done downloading test data')\n",
        "else:\n",
        "  print('Already downloaded test data')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading verbal arithmetic dataset\n",
            "Already downloaded training data\n",
            "Already downloaded test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##  Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab_type": "code",
        "outputId": "1a2aeb1f-fc49-4851-9267-a19852f95888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./PA03_data_20_train.csv\", header=0, names=[\"index\", \"input\", \"label\"])\n",
        "\n",
        "print(\"Number of data points: \", df.shape[0])\n",
        "sampled = df.sample(10)\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points:  640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>264</td>\n",
              "      <td>thirteen plus four</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>712</td>\n",
              "      <td>fifteen minus twelve</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>327</td>\n",
              "      <td>sixteen plus seven</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>58</td>\n",
              "      <td>two plus eighteen</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>646</td>\n",
              "      <td>twelve minus six</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>626</td>\n",
              "      <td>eleven minus six</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>759</td>\n",
              "      <td>seventeen minus nineteen</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>691</td>\n",
              "      <td>fourteen minus eleven</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>372</td>\n",
              "      <td>eighteen plus twelve</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>274</td>\n",
              "      <td>thirteen plus fourteen</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     index                     input  label\n",
              "20     264        thirteen plus four      2\n",
              "180    712      fifteen minus twelve      2\n",
              "270    327        sixteen plus seven      2\n",
              "221     58         two plus eighteen      2\n",
              "542    646          twelve minus six      2\n",
              "143    626          eleven minus six      2\n",
              "247    759  seventeen minus nineteen      0\n",
              "467    691     fourteen minus eleven      2\n",
              "283    372      eighteen plus twelve      2\n",
              "259    274    thirteen plus fourteen      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWzpPi92UAH",
        "colab_type": "text"
      },
      "source": [
        "The two properties we actually care about are the the `inputs` and its `label`, which are the questions and the answers.  \n",
        "\n",
        "**label=0** means the result of expression is **negative**\n",
        "\n",
        "**label=1** means the result of expression is **zero**\n",
        "\n",
        "**label=2** means the result of expression is **positive** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5",
        "colab_type": "text"
      },
      "source": [
        "## BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab_type": "code",
        "outputId": "fb59f4f9-5ddf-44b1-e7c7-45970d19c988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab_type": "code",
        "outputId": "97cbd8a9-7a13-4f0c-83b8-21effd083095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "inputs = df.input.values\n",
        "labels = df.label.values\n",
        "print(\"Train data size \", len(inputs))\n",
        "print(' Original: ', inputs[0])\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(inputs[0]))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(inputs[0])))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data size  640\n",
            " Original:  five minus twelve\n",
            "Tokenized:  ['five', 'minus', 'twelve']\n",
            "Token IDs:  [2274, 15718, 4376]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeNIc4auFUdF",
        "colab_type": "text"
      },
      "source": [
        "We can actually use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww",
        "colab_type": "text"
      },
      "source": [
        "## BERT Required Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDcqNlvVhL5W",
        "colab_type": "text"
      },
      "source": [
        "In a deep learning based NLP pipeline, most of the following preprocessing tricks are frequently needed regardless of whether we use BERT or RNN.\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mceWWOjZnw",
        "colab_type": "text"
      },
      "source": [
        "### Special Tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykk0P9JiKtVe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**`[SEP]`**\n",
        "\n",
        "At the end of every sentence, we need to append the special `[SEP]` token. \n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C9objaKu8f",
        "colab_type": "text"
      },
      "source": [
        "**`[CLS]`**\n",
        "\n",
        "For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n",
        "\n",
        "On the output of the final transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n",
        "\n",
        ">  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\n",
        "corresponding to this token is used as the aggregate sequence representation for classification\n",
        "tasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Also, because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51v0kFxeteu",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Length & Attention Mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNuwqZVK3T6",
        "colab_type": "text"
      },
      "source": [
        "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
        "\n",
        "BERT has two constraints:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. \n",
        "\n",
        "The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ",
        "colab_type": "text"
      },
      "source": [
        "## Sentences to IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV",
        "colab_type": "text"
      },
      "source": [
        "The `tokenizer.encode` function combines multiple steps for us:\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "\n",
        "Oddly, this function can perform truncating for us, but doesn't handle padding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJBVKCyl0HZl",
        "colab_type": "code",
        "outputId": "4e163f8e-e8cf-4b84-8962-d7519d3527b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "# For Verbal Arithmetic\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for input in inputs:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_input = tokenizer.encode(\n",
        "                        input,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_input)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', inputs[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  five minus twelve\n",
            "Token IDs: [101, 2274, 15718, 4376, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwCKszh6ych",
        "colab_type": "text"
      },
      "source": [
        "## Padding & Truncating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0",
        "colab_type": "text"
      },
      "source": [
        "Pad and truncate our sequences so that they all have the same length, `MAX_LEN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqiWTDrn_nGB",
        "colab_type": "text"
      },
      "source": [
        "First, what's the maximum sentence length in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUZO9vc_l6T",
        "colab_type": "code",
        "outputId": "9fea7b81-3630-4cef-e111-71c72e8487f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-54FcQ_p3h",
        "colab_type": "text"
      },
      "source": [
        "Given that, let's choose MAX_LEN = 7 since our numerical expression is quite short. Then apply the padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp9BPRd1tMIo",
        "colab_type": "code",
        "outputId": "cd772f14-c4d1-4348-cb2d-66e64aa65967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "MAX_LEN = 7\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 7 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDs-MYtYH8sL",
        "colab_type": "text"
      },
      "source": [
        "## Attention Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT",
        "colab_type": "text"
      },
      "source": [
        "The attention mask simply makes it explicit which tokens are actual words versus which are padding. \n",
        "\n",
        "The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoC24LeEv3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_",
        "colab_type": "text"
      },
      "source": [
        "## Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06",
        "colab_type": "text"
      },
      "source": [
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-",
        "colab_type": "code",
        "outputId": "5515caa2-4de0-448d-ab66-8fcb145c6f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "print(input_ids)\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "set(labels)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101  2274 15718 ...   102     0     0]\n",
            " [  101  7093  4606 ...   102     0     0]\n",
            " [  101  2416  4606 ...   102     0     0]\n",
            " ...\n",
            " [  101  2809  4606 ...   102     0     0]\n",
            " [  101  2698 15718 ...   102     0     0]\n",
            " [  101  2176  4606 ...   102     0     0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR",
        "colab_type": "text"
      },
      "source": [
        "## Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4",
        "colab_type": "text"
      },
      "source": [
        "Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN",
        "colab_type": "text"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-",
        "colab_type": "text"
      },
      "source": [
        "# 4. Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y_wqqvFhu1dQ"
      },
      "source": [
        "## Question1 [0pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwE1YaCY94Ra",
        "colab_type": "text"
      },
      "source": [
        "The pre-trained neural network here is the normal BERT model from [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). The goal is to add a new classification layer to the pre-trained model. We have provided two example classes to do so.\n",
        "\n",
        "In this part, you need to make your own  `BertCSC413_MLP` class `self.classifier` by, for example, modifying the provided examples: change the number of layers; change the number of hidden neurons; or try a different activation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJKHoftQ_Wsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "class BertCSC413_Linear(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertCSC413_Linear, self).__init__(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "class BertCSC413_MLP_Example(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertCSC413_MLP_Example, self).__init__(config)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "            )\n",
        "\n",
        "class BertCSC413_MLP(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertCSC413_MLP, self).__init__(config)\n",
        "        # Your own classifier goes here\n",
        "        # self.classifier = #TODO Try more layers or different activations \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc",
        "colab_type": "text"
      },
      "source": [
        "## Question2 [0pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2C--cnF_TlD",
        "colab_type": "text"
      },
      "source": [
        "We instantiated two different BERT models from `BertCSC413_MLP` class, which are called `model_freeze_bert` and `model_finetune_bert` in the notebook. \n",
        "\n",
        "**Run** the following code to train the models, and attach the training error curves of `model_freeze_bert` and `model_finetune_bert`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, BertConfig\n",
        "\n",
        "model_freeze_bert = BertCSC413_MLP.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 3, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCrbP2RUjmXO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "for name, param in model_freeze_bert.named_parameters():\n",
        "\tif 'classifier' not in name: # classifier layer\n",
        "\t\tparam.requires_grad = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8UyhbERf0W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_finetune_bert = BertCSC413_MLP.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 3,    \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab_type": "code",
        "outputId": "6e58289c-c936-4abb-8ad5-ee99e9e4ac92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        }
      },
      "source": [
        "# Model parameters visualization\n",
        "params = list(model_finetune_bert.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 203 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.0.weight                                       (768, 768)\n",
            "classifier.0.bias                                             (768,)\n",
            "classifier.2.weight                                         (3, 768)\n",
            "classifier.2.bias                                               (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk",
        "colab_type": "text"
      },
      "source": [
        "We use\n",
        "- Batch size: 32\n",
        "- Learning rate (Adam): 2e-5  \n",
        "- Number of epochs: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBhTDu3jqgeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def train_model(model):      \n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "    epochs = 4\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "    loss_values = []\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input_ids = batch[0] #.to(device)\n",
        "            b_input_mask = batch[1] #.to(device)\n",
        "            b_labels = batch[2] #.to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # This will return the loss (rather than the model output) because we\n",
        "            # have provided the `labels`.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "            \n",
        "            # The call to `model` always returns a tuple, so we need to pull the \n",
        "            # loss value out of the tuple.\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value \n",
        "            # from the tensor.\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "            \n",
        "\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            # batch = tuple(t.to(device) for t in batch)\n",
        "            batch = tuple(t for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            \n",
        "            with torch.no_grad():        \n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask)\n",
        "            \n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    return loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwm_4q5LuHAC",
        "colab_type": "code",
        "outputId": "d154631e-c3ef-4302-e631-28da36f9437a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "source": [
        "freeze_bert_loss_vals = train_model(model_freeze_bert) # about 1 minute for 4 epochs using CPU"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epcoh took: 0:00:10\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:09\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.65\n",
            "  Training epcoh took: 0:00:10\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:09\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX8jzBSluAtA",
        "colab_type": "code",
        "outputId": "ec0928b0-4336-4ccf-ec5d-b3f9226bfbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "source": [
        "finttune_bert_loss_vals = train_model(model_finetune_bert) # about 5 minutes for 4 epochs using CPU"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.71\n",
            "  Training epcoh took: 0:01:04\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:01:04\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:01:03\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:01:02\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "outputId": "b09f8272-c461-43a3-a7cf-2e7ba0daebab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "def plot_loss_vals(loss_vals):\n",
        "    sns.set(style='darkgrid')\n",
        "    sns.set(font_scale=1.5)\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "    plt.plot(loss_vals, 'b-o')\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "plot_loss_vals(freeze_bert_loss_vals)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-402aa461d09a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplot_loss_vals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreeze_bert_loss_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'freeze_bert_loss_vals' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_7H1trGWQ0R",
        "colab_type": "code",
        "outputId": "1a48a2f0-f7e8-4fd5-c847-330a06bee70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plot_loss_vals(finttune_bert_loss_vals)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2t7DwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naNxk-y6Bn1F",
        "colab_type": "text"
      },
      "source": [
        "*Run* the evaluation functions.  Report the test performances of using trained model\\_freeze\\_bert and model\\_finetune\\_bert, and briefly discuss why models are failing under certain target labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab_type": "code",
        "outputId": "7fd9cef1-67d1-40e9-d634-4eed02096a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "#df = pd.read_csv(\"./output.txt_test.csv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df = pd.read_csv(\"./PA03_data_20_test.csv\", header=0, names=[\"index\", \"input\", \"label\"])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "inputs = df.input.values\n",
        "labels = df.label.values\n",
        "\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "input_sent = []\n",
        "\n",
        "# For every sentence...\n",
        "for arithmetic_input in inputs:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        arithmetic_input, \n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    input_sent.append(arithmetic_input)\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_testdata(model_test, show_all_predictions=False):\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model_test.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    predictions , true_labels, input_sents = [], [], []\n",
        "\n",
        "    # Predict \n",
        "    for batch in prediction_dataloader:\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        decoded_inputs = [tokenizer.decode(b_input_ids[i]).strip(\"[CLS] \").strip( \"[SEP] \").strip(\" [PAD] \").strip(\"[PAD]\").strip(\" [SE\") for i in range(len(b_input_ids))]\n",
        "        # Telling the model not to compute or store gradients, saving memory and \n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model_test(b_input_ids, token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        input_sents.extend(decoded_inputs)\n",
        "        \n",
        "        # Store predictions and true labels\n",
        "        predictions.extend(np.argmax(logits, axis=1))\n",
        "        true_labels.extend(label_ids)\n",
        "    correct = [0,0,0]\n",
        "    totals = [0, 0, 0]\n",
        "    for true_label, prediction in zip(true_labels, predictions):\n",
        "        if true_label == prediction:\n",
        "            correct[true_label] += 1\n",
        "        totals[true_label] += 1\n",
        "    print(\"Number of expressions with negative result\", true_labels.count(0), \"\\n\",  correct[0],  \" predicted correctly\",  \", accuracy \", correct[0]/totals[0] , \"\\n\")\n",
        "    print(\"Number of expressions with 0 result\", true_labels.count(1), \"\\n\",  correct[1], \" predicted correctly\", \", accuracy \", correct[1]/totals[1], \"\\n\")\n",
        "    print(\"Number of expressions with positive result\", true_labels.count(2),\"\\n\",  correct[2], \" predicted correctly\",\", accuracy \", correct[2]/totals[2], \"\\n\")\n",
        "    if show_all_predictions:\n",
        "        index_to_sentiment_map = {0:\"negative\", 1:\"zero\", 2:\"positive\"}\n",
        "        for sent in [sent + \"--> \"+index_to_sentiment_map[index]  for sent, index in zip(input_sents, predictions)]:\n",
        "            print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPr4f5hvzAwH",
        "colab_type": "code",
        "outputId": "397932e0-25be-4953-b5e7-1fe4b470da37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "eval_testdata(model_freeze_bert, show_all_predictions=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 160 test sentences...\n",
            "Number of expressions with negative result 47 \n",
            " 0  predicted correctly , accuracy  0.0 \n",
            "\n",
            "Number of expressions with 0 result 2 \n",
            " 0  predicted correctly , accuracy  0.0 \n",
            "\n",
            "Number of expressions with positive result 111 \n",
            " 111  predicted correctly , accuracy  1.0 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-oKTjNrYB8",
        "colab_type": "code",
        "outputId": "d7f8a17c-154c-41e1-989e-14c185cad70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "eval_testdata(model_finetune_bert, show_all_predictions=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 160 test sentences...\n",
            "Number of expressions with negative result 47 \n",
            " 47  predicted correctly , accuracy  1.0 \n",
            "\n",
            "Number of expressions with 0 result 2 \n",
            " 0  predicted correctly , accuracy  0.0 \n",
            "\n",
            "Number of expressions with positive result 111 \n",
            " 109  predicted correctly , accuracy  0.9819819819819819 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik",
        "colab_type": "text"
      },
      "source": [
        "## Question3 [1pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tezCqLQsB1BJ",
        "colab_type": "text"
      },
      "source": [
        "Try a few unseen examples of arithmetic questions using either model\\_freeze\\_bert or model\\_finetune\\_bert model, and find 10 interesting results.  We will give full marks as long as you provide some comments for why you chose some of the examples. The interesting results can, for example, be both successful extrapolation/interpolation results or surprising failure cases.  You can find some examples in our notebook.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJHaMPJk1AG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_to_sentiment_map = {0:\"negative\", 1:\"zero\", 2:\"positive\"}\n",
        "model = model_finetune_bert\n",
        "def what_is(arithmetic_input):\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        arithmetic_input, \n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                )\n",
        "    input_sent = [arithmetic_input]\n",
        "    input_ids = [encoded_sent]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask) \n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(torch.tensor(input_ids), token_type_ids=None, \n",
        "                            attention_mask=torch.tensor(attention_masks))\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        print(index_to_sentiment_map[np.argmax(logits, axis=1)[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I-sf_VlOFTQ",
        "colab_type": "code",
        "outputId": "9483e612-6988-4dde-d9e9-a9d7698f5123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"twelve minus fourteen\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-EyrQtKSJ2W",
        "colab_type": "code",
        "outputId": "66766ac2-bf2a-4a54-de25-7f424e4cf196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"twelve plus fourteen\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5bJMb_NU9Aw",
        "colab_type": "code",
        "outputId": "015944ef-e385-4bc9-f146-88ebc888669d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"eight plus thousand\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkDbHDVhsaMp",
        "colab_type": "code",
        "outputId": "39364f2b-485a-433b-bfa4-b8e6b531b3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"eight minus thousand\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u5YxpjcseDI",
        "colab_type": "code",
        "outputId": "793fb729-1a22-422b-cf81-94ad1528106e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"thousand minus eight\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPckLBdOshTJ",
        "colab_type": "code",
        "outputId": "2d8e8b18-8021-43d6-9bd9-56aea40a2af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"eight minus thousand\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "54f4c0d6-5686-48fa-f986-925ddd4fcab3",
        "id": "6a04x0nNWbO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"1 minus 14\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJttWunQWdA0",
        "colab_type": "code",
        "outputId": "15d3ca6a-ba9a-41b9-b1a1-a484b99b64a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"1 minus two\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4a3KcjuWpmE",
        "colab_type": "code",
        "outputId": "462fd892-7091-4d2f-8bf1-3b347c03ac2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"one minus two\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLoFyJXPWtrV",
        "colab_type": "code",
        "outputId": "0f7f0789-692d-4124-f0e0-bd7f54a0507a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"three minus two minus eight\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-NvEUBJWx6_",
        "colab_type": "code",
        "outputId": "a50d8038-dee7-4b6f-97bd-925470a3e7b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"three minus two\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKm-u-uwW3Rd",
        "colab_type": "code",
        "outputId": "4cb4001b-170d-4dc7-e580-5fe69f576f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"one minus one minus one\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5q21x4YXA6k",
        "colab_type": "code",
        "outputId": "bfba756b-327f-435a-9c0e-9bf4b5ef6b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"one minus one minus one plus ten\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pImh34FXJzu",
        "colab_type": "code",
        "outputId": "aa5b37b8-5fd1-41ae-d7a4-74d9aa2991e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"one minus one plus ten minus one\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGbzrdKTXfVK",
        "colab_type": "code",
        "outputId": "a2d529cb-9a86-4f0a-aa2a-f44f6ef4d1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"minus three plus eight\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6upTtEMmTnJ",
        "colab_type": "code",
        "outputId": "bfd28563-1e79-41ff-bb10-2d27b13bc30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"two minus two\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM_tmB6omgOq",
        "colab_type": "code",
        "outputId": "7036e421-81c8-479c-d805-841e5ae2c9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is(\"thousand minus thousand\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHGLGxaMxPz",
        "colab_type": "code",
        "outputId": "e3c983cc-3f97-48bd-923c-54b97ef39469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is('1 minus 1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eapYfZMjM673",
        "colab_type": "code",
        "outputId": "62629b6c-fbb4-4934-fa5e-0ee85d736af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is('1 plus twelve')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpKUSD5pNOa_",
        "colab_type": "code",
        "outputId": "aa578f54-50a8-4740-f098-4f8a49f88454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "what_is('one plus one minus one')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yhkjxrkNCBF4"
      },
      "source": [
        "## Question4 [1pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4_r_DAtCEJM",
        "colab_type": "text"
      },
      "source": [
        "This is an open question, and we will give marks as long as you show an attempt to try one of the following tasks.   \n",
        "1. Try data augmentation tricks to improve the performances for certain target labels that models were failing to predict.  \n",
        "2. Make a t-sne or PCA plot to visualize the embedding vectors of word tokens related to arithmetic expressions. \n",
        "3. Try different hyperparameter tunings. E.g. learning rates, optimizer, architecture of the classifier, training epochs, and batch size.  \n",
        "4. Evaluate the Multi-class Matthews correlation score for our imbalanced test dataset.  \n",
        "5. Run a baseline model using MLP without pre-trained BERT. You can assume the sequence length of all the data is 3 in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8eLzsg_qFuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, num_epoch, learning_rate):      \n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "    epochs = num_epoch\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "    loss_values = []\n",
        "\n",
        "    max_val_accuracy = 0\n",
        "    for epoch_i in range(0, epochs):\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input_ids = batch[0] #.to(device)\n",
        "            b_input_mask = batch[1] #.to(device)\n",
        "            b_labels = batch[2] #.to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # This will return the loss (rather than the model output) because we\n",
        "            # have provided the `labels`.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "            \n",
        "            # The call to `model` always returns a tuple, so we need to pull the \n",
        "            # loss value out of the tuple.\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value \n",
        "            # from the tensor.\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "            \n",
        "\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            # batch = tuple(t.to(device) for t in batch)\n",
        "            batch = tuple(t for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            \n",
        "            with torch.no_grad():        \n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask)\n",
        "            \n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "        \n",
        "        if eval_accuracy > max_val_accuracy:\n",
        "          max_val_accuracy = eval_accuracy\n",
        "\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    return loss_values, max_val_accuracy/nb_eval_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwgbUrzqSsE",
        "colab_type": "code",
        "outputId": "cbe90d33-18f2-441b-bf27-c90c2ae21e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_rates = [1e-5, 2e-5, 3e-5, 4e-5]\n",
        "num_epochs = [5, 6, 7, 8, 9, 10]\n",
        "\n",
        "best_hyper = [None, None]\n",
        "best_accuracy = 0\n",
        "for learning_rate in learning_rates:\n",
        "  for num_epoch in num_epochs:\n",
        "    finttune_bert_loss_vals = train_model(model_finetune_bert, num_epoch, learning_rate)\n",
        "    loss_values, val_accuracy = finttune_bert_loss_vals\n",
        "    if val_accuracy > best_accuracy:\n",
        "      best_hyper = [learning_rate, num_epoch]\n",
        "      best_accuracy = val_accuracy\n",
        "      print('best hyperparameter:', best_hyper)\n",
        "      print('best validation accuracy:', best_accuracy)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:01:13\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epcoh took: 0:01:14\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:01:14\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:01:12\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "best hyperparameter: [1e-05, 5]\n",
            "best validation accuracy: 0.953125\n",
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:01:15\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:15\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "best hyperparameter: [1e-05, 6]\n",
            "best validation accuracy: 0.984375\n",
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:01:23\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "best hyperparameter: [1e-05, 7]\n",
            "best validation accuracy: 1.0\n",
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:23\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:23\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:22\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:15\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:14\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:14\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:01:14\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:13\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:13\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:15\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:15\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:24\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:24\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:23\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:23\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:21\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:20\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:19\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 7 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:16\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 9 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:17\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:18\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1dkhaBpqwAl",
        "colab_type": "code",
        "outputId": "54a08c94-e32f-4c96-9a33-29f684b1ee3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plot_loss_vals(finttune_bert_loss_vals)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAGaCAYAAAB+A+cSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAzQ/wfsDFmToILJMwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjU35FzotcYM",
        "colab_type": "code",
        "outputId": "ea9ec604-ede4-4c41-e758-4f17b8a5d20f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "eval_testdata(model_finetune_bert, show_all_predictions=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 160 test sentences...\n",
            "Number of expressions with negative result 47 \n",
            " 47  predicted correctly , accuracy  1.0 \n",
            "\n",
            "Number of expressions with 0 result 2 \n",
            " 0  predicted correctly , accuracy  0.0 \n",
            "\n",
            "Number of expressions with positive result 111 \n",
            " 111  predicted correctly , accuracy  1.0 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}